{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vgG0UORh0mri"
   },
   "source": [
    "# Extracting mastergrid-aligned covariates from Earth Engine\n",
    "\n",
    "This means covariates which are at exactly 30 arcsecond (\"1km\") / 2.5 arcminute (\"5km\") resolutions, or clean multiples/fractions of this, and with global extent or pixels that are aligned to those of a global grid (one with origin at -180, 90)\n",
    "\n",
    "## First install and authenticate the Earth Engine python API.\n",
    "\n",
    "This needs to be done each time for running in colab, as it's a new runtime each time. Don't run these cells if running in a local notebook server.\n",
    "\n",
    "Ensure that you authenticate using a google account which has access to Earth Engine (this is most likely your personal account rather than your MAP G-Suite account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 7396,
     "status": "ok",
     "timestamp": 1559726479283,
     "user": {
      "displayName": "Harry Gibson",
      "photoUrl": "",
      "userId": "07981519042514161991"
     },
     "user_tz": -60
    },
    "id": "zGysDq3U0FRf",
    "outputId": "260d24cb-b732-48b0-a2e6-ab87a3f35878"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting earthengine-api\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/df/5a/11b0ccdee986474f2b8ec6d2730a6ce883eeb37f4e5fc16311964b0e916a/earthengine-api-0.1.180.tar.gz (135kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 4.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: google-api-python-client in /usr/local/lib/python3.6/dist-packages (from earthengine-api) (1.6.7)\n",
      "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from earthengine-api) (1.4.2)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from earthengine-api) (0.0.3)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from earthengine-api) (0.11.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from earthengine-api) (1.12.0)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client->earthengine-api) (3.0.0)\n",
      "Requirement already satisfied: oauth2client<5.0.0dev,>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client->earthengine-api) (4.1.3)\n",
      "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->earthengine-api) (3.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->earthengine-api) (0.2.5)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->earthengine-api) (4.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client<5.0.0dev,>=1.5.0->google-api-python-client->earthengine-api) (0.4.5)\n",
      "Building wheels for collected packages: earthengine-api\n",
      "  Building wheel for earthengine-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Stored in directory: /root/.cache/pip/wheels/57/01/bd/c8c309e42c1d463e475c78fc3393b749e5a643bbb39f367a0b\n",
      "Successfully built earthengine-api\n",
      "Installing collected packages: earthengine-api\n",
      "Successfully installed earthengine-api-0.1.180\n"
     ]
    }
   ],
   "source": [
    "!pip install earthengine-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 24114,
     "status": "ok",
     "timestamp": 1559726498604,
     "user": {
      "displayName": "Harry Gibson",
      "photoUrl": "",
      "userId": "07981519042514161991"
     },
     "user_tz": -60
    },
    "id": "IdcYg9Ksz6Tz",
    "outputId": "19920ed4-56b0-47db-9e69-fbef62493ece"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command using Cloud API.  Set --no-use_cloud_api to go back to using the API\n",
      "Opening the following address in a web browser:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?client_id=517222506229-vsmmajv00ul0bs7p89v5m89qs8eb9359.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fearthengine+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code\n",
      "\n",
      "Please authorize access to your Earth Engine account, and paste the generated code below. If the web browser does not start, please manually browse the URL above.\n",
      "\n",
      "Please enter authorization code: 4/YAFYly9Ks8fDm0SneAgN3Bq3HY6G28oahQC4mTbAR0lzt3SuaMzXmHU\n",
      "\n",
      "Successfully saved authorization token.\n"
     ]
    }
   ],
   "source": [
    "!earthengine authenticate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2V0yKSYW1bTQ"
   },
   "source": [
    "## Import the EE API and check the install \n",
    "\n",
    "Check that everything works correctly by referencing an image from Earth Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 998,
     "status": "ok",
     "timestamp": 1559743282408,
     "user": {
      "displayName": "Harry Gibson",
      "photoUrl": "",
      "userId": "07981519042514161991"
     },
     "user_tz": -60
    },
    "id": "fZDM7U2gz20v",
    "outputId": "fbe21297-d67c-4e8f-b96c-3c98e9c585cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'Image', 'bands': [{'id': 'elevation', 'data_type': {'type': 'PixelType', 'precision': 'int', 'min': -32768, 'max': 32767}, 'dimensions': [432000, 144000], 'crs': 'EPSG:4326', 'crs_transform': [0.000833333333333, 0.0, -180.0, 0.0, -0.000833333333333, 60.0]}], 'version': 1494271934303000, 'id': 'srtm90_v4', 'properties': {'system:time_start': 950227200000, 'system:time_end': 951177600000, 'system:asset_size': 18827626666}}\n"
     ]
    }
   ],
   "source": [
    "# Import the Earth Engine Python Package\n",
    "import ee\n",
    "\n",
    "# Initialize the Earth Engine object, using the authentication credentials.\n",
    "ee.Initialize()\n",
    "\n",
    "# Print the information for an image asset.\n",
    "image = ee.Image('srtm90_v4')\n",
    "print(image.getInfo())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some globals for our setup in MAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "iR-jlTUY0kE1"
   },
   "outputs": [],
   "source": [
    "# Resolutions for our lat/lon \"mastgergrids, defining what they get informally \n",
    "# referred to as vs their actual resolution in degrees. The keys are used in \n",
    "# the mastergrid filename syntax\n",
    "mgResolutions = {\n",
    "    \"30m\"  : 1.0/3600.0,\n",
    "    \"100m\" : 1.0/1200.0,\n",
    "    \"500m\" : 1.0/240.0,\n",
    "    \"1km\"  : 1.0/120.0,\n",
    "    \"5km\"  : 1.0/24.0,\n",
    "    \"10km\" : 1.0/12.0,\n",
    "    \"Unchanged\" : None\n",
    "}\n",
    "\n",
    "# The main extents that we use / will want to export at, include any data specific \n",
    "# ones here for datasets that aren't global, in particular less than +- 90deg latitude\n",
    "mgRegions = {\n",
    "    \"nearlyGlobal\": ee.Geometry.Rectangle(**{\n",
    "        'coords'   : [-180, -60, 180, 85],\n",
    "        'geodesic' : False,\n",
    "        'proj'     : 'EPSG:4326'\n",
    "    }),\n",
    "    \"global\": ee.Geometry.Rectangle(**{\n",
    "        'coords'   : [-180, -90, 180, 90],\n",
    "        'geodesic' : False,\n",
    "        'proj'     : 'EPSG:4326'\n",
    "    }),\n",
    "    \"viirs-extent\": ee.Geometry.Rectangle(**{\n",
    "        'coords'   : [-180, -65, 180, 75],\n",
    "        'geodesic' : False,\n",
    "        'proj'     : 'EPSG:4326'\n",
    "    }),\n",
    "    \"viirs-extent-7560\": ee.Geometry.Rectangle(**{\n",
    "        'coords'   : [-180, -60, 180, 75],\n",
    "        'geodesic' : False,\n",
    "        'proj'     : 'EPSG:4326'\n",
    "    })\n",
    "    ,\"mapAfrica\" : ee.Geometry.Rectangle(**{\n",
    "        'coords'   : [-18, -35, 52, 37.5],\n",
    "        'geodesic' : False,\n",
    "        'proj'     : 'EPSG:4326'\n",
    "    })\n",
    "}\n",
    "\n",
    "# The different timesteps at which we store temporal aggregations of dynamic \n",
    "# datasets. These strings will just be used to build filenames\n",
    "mgTimesteps = [\"Monthly\", \"Annual\", \"Synoptic_Overall\", \"Synoptic\"]\n",
    "\n",
    "# The different continuous aggregation types that are available; matches the keys \n",
    "# returned by the spatialSummaries_Continuous function and will be used directly \n",
    "# in filename generation. 'Data' means no aggregation\n",
    "continuousAggregationStats = ['min', 'max', 'mean', 'SD', 'Data']\n",
    "categoricalAggregationStats = ['majority', 'fraction', 'Data'] # 'like'\n",
    "\n",
    "# The cloud storage bucket to which we should export. The user account used to authenticate \n",
    "# to Earth Engine must have at least write-access to this bucket, so set up your own one \n",
    "# if necessary.\n",
    "EXPORT_BUCKET = 'map-ee-outputs'\n",
    "\n",
    "\n",
    "# The maximum dimensions for an exported file before breaking it into tiles. \n",
    "# This random-seeming pair of numbers is based on a global \"500m\" (15 arcsecond) grid, which is \n",
    "# 86400*43200. However they need to be a multiple of 256 (the default for shardSize), so these \n",
    "# are such and a-bit-of-leeway beyond.\n",
    "# Keen to avoid tiling when possible to reduce faff (e.g. would have to regenerate pyramids locally) \n",
    "# but beyond a certain size it becomes unreliable. Global 30 arcsecond grids seem to mostly \n",
    "# work but global 15 arcsecond ones only occasionally do.\n",
    "\n",
    "#MAX_DIMS = [89856,44800]    \n",
    "\n",
    "MAX_DIMS = [43264,21760]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define EE processing functions\n",
    "\n",
    "### Spatial aggregation of continuous-type data\n",
    "\n",
    "For such data we might want to perform spatial aggregation based on min / max / mean / SD \n",
    "\n",
    "Note the hardcoded nCells - we want to force Earth Engine to use all data pixels in producing an \n",
    "output pixel. This value tells it the max number of inputs per output to look for. It would be \n",
    "unlikely that we'd want to aggregate say 30m data to 1km or coarser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "V0XzUY2l6BON"
   },
   "outputs": [],
   "source": [
    "def spatialSummaries_Continuous(img):\n",
    "    # Because of lazy evaluation we specify the means by which an image _will_ be \n",
    "    # reduced when actually required at a lower resolution, but we don't specify \n",
    "    # any particular resolution at this point. So the function simply returns them \n",
    "    # all, and they will only be made flesh if and when exported.\n",
    "    minRed = ee.Reducer.min()\n",
    "    maxRed = ee.Reducer.max()\n",
    "    meanRed = ee.Reducer.mean()\n",
    "    sdRed = ee.Reducer.sampleStdDev()\n",
    "    \n",
    "    nCells = 20 * 20 * 1.05\n",
    "    return {\n",
    "        \"min\"  : img.reduceResolution(minRed, False, nCells),\n",
    "        \"max\"  : img.reduceResolution(maxRed, False, nCells),\n",
    "        \"mean\" : img.reduceResolution(meanRed, False, nCells),\n",
    "        \"SD\"   : img.reduceResolution(sdRed, False, nCells),\n",
    "        \"Data\" : img\n",
    "    }\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial aggregation of categorical-type data\n",
    "\n",
    "For such data in MAP we produce:\n",
    "* A single grid with the modal input value at each output pixel\n",
    "* A grid for each discrete input value, giving at each output pixel the percentage of input pixels that had that value\n",
    "* A grid for each discrete input value, giving at each output pixel the \"like-adjacency\" of the input pixels that had that value. This is a measure of how many of the 8 neighbours of each pixel have the same value as the pixel itself. We calculate this for each source pixel, and then take the mean value across the output pixels. We have not yet implemented this in Earth Engine and will probably instead do this using the entropy() function if needed, but for now I don't think these grids ever really get used.\n",
    "\n",
    "Note that as well as nCells, defined as before, we also hardcode a maximum number of categories that the input data can have (and thus the number of output percentage grids there will be)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "VCaQ_FGMpVrK"
   },
   "outputs": [],
   "source": [
    "def spatialSummaries_Categorical(img, categoryMapping = None):#categoryValues=None, categoryNames=None):\n",
    "    '''Produces a majority (mode) value aggregation as well as one percentage-cover aggregation \n",
    "    for each specified pixel value.\n",
    "    img = the ee.Image to aggregate, should be categorical data for results to be meaningful\n",
    "    categoryMapping = optional, a list of 2-tuples which are (value, categoryName).\n",
    "    If categoryMapping is provided then output will contain a \"Fractions\" item, the value of \n",
    "    which is another dictionary, mapping \n",
    "        categoryName:(ee.Image giving the fractional cover of that value).\n",
    "    Output will always contain a \"Majority\" item, and a \"Data\" item which is the unmodified image \n",
    "    (and will give nearest-neighbour aggregation on export).'''\n",
    "    \n",
    "    nCells = 20 * 20 * 1.05\n",
    "    modeRed = img.reduceResolution(ee.Reducer.mode(maxRaw=100), False, nCells)\n",
    "    outObj = {\n",
    "        \"majority\" : modeRed,\n",
    "        \"Data\": img\n",
    "    }\n",
    "    if categoryMapping is not None:\n",
    "        if len(categoryMapping) < 2 or len(categoryMapping)>100:\n",
    "            raise ValueError(\"If categories are provided for proportional outputs, there must be 2<=n<=100 categories\")\n",
    "        # assuming we have a list of [(k,v),(k2,v2)] then this will give [(k,k2),(v,v2)] and immediately unpack it \n",
    "        categoryValues, categoryNames = list(zip(*categoryMapping))\n",
    "        binaryImg = ee.Image(categoryValues)\n",
    "        filteredClassProps = (img.eq(binaryImg).rename(categoryNames)\n",
    "                           .reduceResolution(ee.Reducer.mean(), False, nCells))\n",
    "        fractions = {}\n",
    "        for categoryName in categoryNames:\n",
    "            fractions[categoryName] = filteredClassProps.select(categoryName)\n",
    "        outObj[\"fractions\"] = fractions\n",
    "    return outObj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To export cleanly-aligned grids we will use the crs and crsTransform parameters to the EE export function. The crsTransform is the same as the \"geotransform\" as reported/used by GDAL; it needs the location of the origin (top left corner) of the grid. To programatically get the origin in client-side numbers from an EE geometry is a bit of a faff, we could just code them manually but here we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "WdPfdO5AN78N"
   },
   "outputs": [],
   "source": [
    "# seems to be a ridiculous amount of effort just to get the flippin origin points numerically but whatev\n",
    "# The problem is that we don't know how many levels of nesting there will be in coords\n",
    "# depending on whether it's multipart or not so we need to recursively flatten the list\n",
    "def getOriginXY(eeGeom):\n",
    "    from collections import Iterable\n",
    "    def flatten(items):\n",
    "        for x in items:\n",
    "            if isinstance(x, Iterable) and not isinstance(x, (str, bytes)):\n",
    "                for sub_x in flatten(x):\n",
    "                    yield sub_x\n",
    "            else:\n",
    "                yield x\n",
    "    rawCoords = eeGeom.getInfo()['coordinates']\n",
    "    allCoords = list(flatten(rawCoords))\n",
    "    xCoords = allCoords[::2]\n",
    "    yCoords = allCoords[1::2]\n",
    "    minX = min(xCoords)\n",
    "    maxY = max(yCoords)\n",
    "    return (minX, maxY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to create the necessary parameters to create one export Task for each image in a given collection, giving the exported images a name appropriate for the MAP mastergrid filename syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "GKakJByp-CH0"
   },
   "outputs": [],
   "source": [
    "def getFilenameTimeTags(img, mgTimeStep):\n",
    "    # build the year/month parts of the filename\n",
    "    if mgTimeStep.startswith(\"Synoptic\"):\n",
    "        imgYearTag = \"Synoptic\"\n",
    "    else:\n",
    "        imgYearTag = ee.Date(img.get('system:time_start')).get('year').getInfo()\n",
    "    if mgTimeStep == \"Annual\":\n",
    "        imgMonthTag = \"Annual\"\n",
    "    elif mgTimeStep == \"Synoptic_Overall\":\n",
    "        imgMonthTag = \"Overall\"\n",
    "    else:\n",
    "        # this assumes that synoptic-monthly data have a proper date set i.e. \n",
    "        # with an arbitrary year\n",
    "        imgMonthTag = str(ee.Date(img.get('system:time_start')).get('month').getInfo()).zfill(2)\n",
    "    return(imgYearTag, imgMonthTag)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCollExportParams_Raw(collection, mgTimeStepName, timeSummaryName, \n",
    "                             resolutionName, variableName, regionGeom):\n",
    "    '''\n",
    "    collection: an ee.ImageCollection, which must already be aggregated to monthly or annual or whatever\n",
    "    mgTimeStepName: a string representing the timestep as it should be shown in the filenames, e.g. \"Monthly\"\n",
    "    timeSummaryName: a string representing how the images of this timestep were created, e.g. if the images \n",
    "        are monthly and are the mean of daily data then \"mean\", or if they are monthly from monthly data \n",
    "        then \"Data\"\n",
    "    resolutionName: a string representing the mastergrid-named resolution as it should be shown in the filenames \n",
    "        corresponding to a specific decimal degrees resolution\n",
    "    varName: for the first part of the filenames\n",
    "    regionGeom: an ee.Geometry(.Rectangle, preferably)\n",
    "    aggregations: \n",
    "    '''\n",
    "    fnTemplate = \"{0!s}.{1!s}.{2!s}.{3!s}.{4!s}.{5!s}\"\n",
    "    if mgTimeStepName not in mgTimesteps:\n",
    "        raise ValueError(\"specified timestep string isn't valid\")\n",
    "    #if resolutionName not in mgResolutions:\n",
    "    #    raise ValueError(\"specified resolution string isn't valid\")\n",
    "    \n",
    "    exportOrigin = getOriginXY(regionGeom)\n",
    "    #exportRes = mgResolutions[mgResolutionName] # None if mgResolutionName==\"Unchanged\"\n",
    "    \n",
    "    nImages = collection.size().getInfo()\n",
    "    collectionList = collection.toList(nImages) # necessary to allow client-side iterating over them\n",
    "    exportImages = {}\n",
    "        \n",
    "    for i in range(nImages):\n",
    "        img = ee.Image(collectionList.get(i))\n",
    "        imgYearTag, imgMonthTag = getFilenameTimeTags(img, mgTimeStepName)\n",
    "      \n",
    "        # export at the image's own nominal scale, but reprojected (if not already) to EPSG:4326 \n",
    "        # Any resampling needed to do this will be done by the EE default method of nearest-neigbour.\n",
    "        # This doesn't necessarily give us exactly what we want for our mastergrid alignments.\n",
    "        # No spatial aggregation, just one image out for each image in.\n",
    "        imgTrans = img.projection().getInfo()['transform']\n",
    "        existingResX = imgTrans[0]\n",
    "        existingResY = imgTrans[4]\n",
    "        img = img.reproject(ee.Projection('EPSG:4326'), None, scale=existingResX)\n",
    "        affine = str([existingResX, 0.0, exportOrigin[0], 0, existingResY, exportOrigin[1]])\n",
    "        if variableName is None:\n",
    "            varNameOut = aggImg.get('varname').getInfo()\n",
    "        else:\n",
    "            varNameOut = variableName\n",
    "            \n",
    "        fileName = fnTemplate.format(varNameOut, imgYearTag, imgMonthTag, \n",
    "                                     timeSummaryName, resolutionName, \"Data\")\n",
    "        taskDesc = fileName.replace('.', '_')\n",
    "\n",
    "        exportParams = {\n",
    "            'image':img,\n",
    "            'fileNamePrefix': fileName,\n",
    "            'description':taskDesc,\n",
    "            'bucket':EXPORT_BUCKET,\n",
    "            'crs':'EPSG:4326',\n",
    "            'crsTransform':affine,\n",
    "            'region':regionGeom.getInfo()['coordinates'],\n",
    "            'maxPixels':4e9,\n",
    "            'formatOptions':{\n",
    "                'cloudOptimized':True  # ensures internal tiling and creates pyramids\n",
    "            },\n",
    "            # this random-seeming pair of numbers are a multiple of 256 (the default for shardSize)\n",
    "            # that's a-bit-of-leeway bigger than a global 500m grid, which is 86400*43200. This \n",
    "            # ensures that images up to this resolution will be exported as a single file, so no need \n",
    "            # to faff with mosaicing and re-pyramiding the downloads\n",
    "            'fileDimensions':MAX_DIMS\n",
    "        }\n",
    "        if fileName in exportImages:\n",
    "            raise ValueError(\"{} would be a duplicate filename, check collection is what you intended\"\n",
    "                             .format(fileName))\n",
    "        exportImages[fileName] = exportParams\n",
    "\n",
    "    return exportImages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCollExportParams_CatAgg(collection, mgTimeStepName, timeSummaryName,\n",
    "                              mgResolutionName, variableName, regionGeom,\n",
    "                              aggregations=[\"Data\"],\n",
    "                               categoryMapping=None):\n",
    "    fnTemplate = \"{0!s}.{1!s}.{2!s}.{3!s}.{4!s}.{5!s}\"\n",
    "    if mgTimeStepName not in mgTimesteps:\n",
    "        raise ValueError(\"specified timestep string isn't valid\")\n",
    "    if mgResolutionName not in mgResolutions:\n",
    "        raise ValueError(\"specified resolution string isn't valid\")\n",
    "    \n",
    "    exportOrigin = getOriginXY(regionGeom)\n",
    "    exportRes = mgResolutions[mgResolutionName] # None if mgResolutionName==\"Unchanged\"\n",
    "    \n",
    "    nImages = collection.size().getInfo()\n",
    "    collectionList = collection.toList(nImages) # necessary to allow client-side iterating over them\n",
    "    exportImages = {}\n",
    "    for i in range(nImages):\n",
    "        img = ee.Image(collectionList.get(i))\n",
    "        imgYearTag, imgMonthTag = getFilenameTimeTags(img, mgTimeStepName)\n",
    "      \n",
    "        # we are applying some level of spatial aggregation, this might mean we are \n",
    "        # deliberately aggregating e.g. to 5k outputs, but this also applies even \n",
    "        # if we are exporting a true 1km image (e.g. MODIS) to mastergrid \"1km\".\n",
    "        # In this latter case we don't want to create and export min/max/mean/sd aggregations,\n",
    "        # just a standard nearest-neighbour.\n",
    "        affine = str([exportRes, 0.0, exportOrigin[0], 0, -exportRes, exportOrigin[1]])\n",
    "        aggregatedImages = spatialSummaries_Categorical(img, categoryMapping)\n",
    "        for spatialSummary, aggImg in aggregatedImages.items():\n",
    "            if spatialSummary not in aggregations:\n",
    "                continue\n",
    "            if variableName is None:\n",
    "                varNameOut = aggImg.get('varname').getInfo()\n",
    "            else:\n",
    "                varNameOut = variableName\n",
    "            \n",
    "            if spatialSummary == \"Data\":\n",
    "                # User has requested a no-aggregation type summary. Check whether the requested resolution \n",
    "                # for the export is actually close to the native image resolution. If it is, then the spatial \n",
    "                # summary indicator in the filename will be \"Data\". If it's not, then warn the user as this might \n",
    "                # be an error, and record the spatial summary as \"NN\" (nearest neighbour), which is what it will \n",
    "                # default to.\n",
    "                existingProj = img.projection().getInfo()\n",
    "                if existingProj['crs'] != \"EPSG:4326\":\n",
    "                    llProj =  ee.Projection(\"EPSG:4326\").atScale(img.projection().nominalScale())\n",
    "                    reProjWarn = \"(when reprojected to EPSG:4326) \"\n",
    "                else:\n",
    "                    llProj = existingProj\n",
    "                    reProjWarn = \"\"\n",
    "                imgTrans = llProj.getInfo()['transform']\n",
    "                existingResX = imgTrans[0]\n",
    "                factor = max([exportRes, existingResX]) / min([exportRes, existingResX])\n",
    "                if factor > 1.25 and existingResX != 1.0:\n",
    "                    warnings.warn((\"The requested mastergrid resolution of {} differs from \"+\n",
    "                                  \"the image native resolution {}of {} by quite a lot, but no \"+\n",
    "                                  \"spatial summary was requested (aggregations=['Data']). \" +\n",
    "                                  \"Therefore the outputs will be produced by nearest neighbour \" +\n",
    "                                   \"resampling.\").format(\n",
    "                                      exportRes, reProjWarn, existingResX))\n",
    "                    spatialSummary=\"NN\"\n",
    "            if spatialSummary == \"majority\":\n",
    "                spatialSummary = \"majority-class\"\n",
    "            if spatialSummary == \"fractions\":\n",
    "                for className, aggSubImg in aggImg.items():\n",
    "                    varNameOut = className\n",
    "                    fileName = fnTemplate.format(varNameOut, imgYearTag, imgMonthTag,\n",
    "                                                timeSummaryName, mgResolutionName, spatialSummary)\n",
    "                    taskDesc = fileName.replace('.', '_')\n",
    "                    exportParams = {\n",
    "                        'image':aggSubImg,\n",
    "                        'fileNamePrefix': fileName,\n",
    "                        'description':taskDesc,\n",
    "                        'bucket':EXPORT_BUCKET,\n",
    "                        'crs':'EPSG:4326',\n",
    "                        'crsTransform':affine,\n",
    "                        'region':regionGeom.getInfo()['coordinates'],\n",
    "                        'maxPixels':4e9,\n",
    "                        'formatOptions':{\n",
    "                            'cloudOptimized':True # ensures internal tiling and creates pyramids\n",
    "                        },\n",
    "                        'fileDimensions':MAX_DIMS\n",
    "                    }\n",
    "                    if fileName in exportImages:\n",
    "                        raise ValueError(\"{} would be a duplicate filename, check collection is what you intended\"\n",
    "                                         .format(fileName))\n",
    "                    exportImages[fileName] = exportParams\n",
    "            else:    \n",
    "                fileName = fnTemplate.format(varNameOut, imgYearTag, imgMonthTag, \n",
    "                                             timeSummaryName, mgResolutionName, spatialSummary)\n",
    "                taskDesc = fileName.replace('.', '_')\n",
    "                exportParams = {\n",
    "                    'image':aggImg,\n",
    "                    'fileNamePrefix': fileName,\n",
    "                    'description':taskDesc,\n",
    "                    'bucket':EXPORT_BUCKET,\n",
    "                    'crs':'EPSG:4326',\n",
    "                    'crsTransform':affine,\n",
    "                    'region':regionGeom.getInfo()['coordinates'],\n",
    "                    'maxPixels':4e9,\n",
    "                    'formatOptions':{\n",
    "                        'cloudOptimized':True # ensures internal tiling and creates pyramids\n",
    "                    },\n",
    "                    'fileDimensions':MAX_DIMS\n",
    "                }\n",
    "                if fileName in exportImages:\n",
    "                    raise ValueError(\"{} would be a duplicate filename, check collection is what you intended\"\n",
    "                                     .format(fileName))\n",
    "                exportImages[fileName] = exportParams\n",
    "    return exportImages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "def getCollExportParams_ContAgg(collection, mgTimeStepName, timeSummaryName, \n",
    "                                mgResolutionName, variableName, regionGeom,\n",
    "                                aggregations=[\"Data\"]):\n",
    "    '''\n",
    "    collection: an ee.ImageCollection, which must already be aggregated to monthly or annual or whatever\n",
    "    mgTimeStepName: a string representing the timestep as it should be shown in the filenames, e.g. \"Monthly\"\n",
    "    timeSummaryName: a string representing how the images of this timestep were created, e.g. if the images \n",
    "        are monthly and are the mean of daily data then \"mean\", or if they are monthly from monthly data \n",
    "        then \"Data\"\n",
    "    resolutionName: a string representing the mastergrid-named resolution as it should be shown in the filenames \n",
    "        corresponding to a specific decimal degrees resolution\n",
    "    variableName: for the first part of the filenames. If None, then image property \"varname\" will be used.\n",
    "    regionGeom: an ee.Geometry(.Rectangle, preferably)\n",
    "    aggregations: \n",
    "    '''\n",
    "    fnTemplate = \"{0!s}.{1!s}.{2!s}.{3!s}.{4!s}.{5!s}\"\n",
    "    if mgTimeStepName not in mgTimesteps:\n",
    "        raise ValueError(\"specified timestep string isn't valid\")\n",
    "    if mgResolutionName not in mgResolutions:\n",
    "        raise ValueError(\"specified resolution string isn't valid\")\n",
    "    \n",
    "    exportOrigin = getOriginXY(regionGeom)\n",
    "    exportRes = mgResolutions[mgResolutionName] # None if mgResolutionName==\"Unchanged\"\n",
    "    \n",
    "    nImages = collection.size().getInfo()\n",
    "    collectionList = collection.toList(nImages) # necessary to allow client-side iterating over them\n",
    "    exportImages = {}\n",
    "        \n",
    "    for i in range(nImages):\n",
    "        img = ee.Image(collectionList.get(i))\n",
    "        imgYearTag, imgMonthTag = getFilenameTimeTags(img, mgTimeStepName)\n",
    "      \n",
    "        # we are applying some level of spatial aggregation, this might mean we are \n",
    "        # deliberately aggregating e.g. to 5k outputs, but this also applies even \n",
    "        # if we are exporting a true 1km image (e.g. MODIS) to mastergrid \"1km\".\n",
    "        # In this latter case we don't want to create and export min/max/mean/sd aggregations,\n",
    "        # just a standard nearest-neighbour.\n",
    "        affine = str([exportRes, 0.0, exportOrigin[0], 0, -exportRes, exportOrigin[1]])\n",
    "        aggregatedImages = spatialSummaries_Continuous(img)\n",
    "        for spatialSummary, aggImg in aggregatedImages.items():\n",
    "            if spatialSummary not in aggregations:\n",
    "                continue\n",
    "            if variableName is None:\n",
    "                varNameOut = aggImg.get('varname').getInfo()\n",
    "            else:\n",
    "                varNameOut = variableName\n",
    "            if spatialSummary == \"Data\":\n",
    "                # User has requested a no-aggregation type summary. Check whether the requested resolution \n",
    "                # for the export is actually close to the native image resolution. If it is, then the spatial \n",
    "                # summary indicator in the filename will be \"Data\". If it's not, then warn the user as this might \n",
    "                # be an error, and record the spatial summary as \"NN\" (nearest neighbour), which is what it will \n",
    "                # default to.\n",
    "                existingProj = img.projection().getInfo()\n",
    "                if existingProj['crs'] != \"EPSG:4326\":\n",
    "                    llProj =  ee.Projection(\"EPSG:4326\").atScale(img.projection().nominalScale()).getInfo()\n",
    "                    reProjWarn = \"(when reprojected to EPSG:4326) \"\n",
    "                else:\n",
    "                    llProj = existingProj\n",
    "                    reProjWarn = \"\"\n",
    "                imgTrans = llProj['transform']\n",
    "                existingResX = imgTrans[0]\n",
    "                factor = max([exportRes, existingResX]) / min([exportRes, existingResX])\n",
    "                if factor > 1.25 and existingResX != 1.0:\n",
    "                    # when we create a composite from an imageCollection e.g. temporal mean or std,\n",
    "                    # it seems to return a default projection of EPSG:4326, scale 1.0, even if the \n",
    "                    # inputs were e.g. sinusoidal (MODIS), so this check breaks. \n",
    "                    warnings.warn((\"The requested mastergrid resolution of {} differs from \"+\n",
    "                                  \"the image native resolution {}of {} by quite a lot, but no \"+\n",
    "                                  \"spatial summary was requested (aggregations=['Data']). \" +\n",
    "                                  \"Therefore the outputs will be produced by nearest neighbour \" +\n",
    "                                   \"resampling.\").format(\n",
    "                                      exportRes, reProjWarn, existingResX))\n",
    "                    spatialSummary=\"NN\"\n",
    "            fileName = fnTemplate.format(varNameOut, imgYearTag, imgMonthTag, \n",
    "                                         timeSummaryName, mgResolutionName, spatialSummary)\n",
    "            taskDesc = fileName.replace('.', '_')\n",
    "            exportParams = {\n",
    "                'image':aggImg,\n",
    "                'fileNamePrefix': fileName,\n",
    "                'description':taskDesc,\n",
    "                'bucket':EXPORT_BUCKET,\n",
    "                'crs':'EPSG:4326',\n",
    "                'crsTransform':affine,\n",
    "                'region':regionGeom.getInfo()['coordinates'],\n",
    "                'maxPixels':4e9,\n",
    "                'formatOptions':{\n",
    "                    'cloudOptimized':True # ensures internal tiling and creates pyramids\n",
    "                },\n",
    "                'fileDimensions':MAX_DIMS\n",
    "            }\n",
    "            if fileName in exportImages:\n",
    "                raise ValueError(\"{} would be a duplicate filename, check collection is what you intended\"\n",
    "                                 .format(fileName))\n",
    "            exportImages[fileName] = exportParams\n",
    "    return exportImages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def createTasks(taskParamsDict):\n",
    "    taskList = []\n",
    "    for _, params in taskParamsDict.items():\n",
    "        task = ee.batch.Export.image.toCloudStorage(**params)\n",
    "        taskList.append(task)\n",
    "    return taskList\n",
    "\n",
    "def performTasks(taskList):\n",
    "    for t in taskList:\n",
    "        t.start()\n",
    "    \n",
    "def reportTasks(taskList):\n",
    "    stati = [] # yeah no i've no idea\n",
    "    for t in taskList:\n",
    "        status = t.status()\n",
    "        out = status['description'] + \": \" +status['state'] \n",
    "        if 'start_timestamp_ms' in status:\n",
    "            timeSecs = (status['update_timestamp_ms'] - status['start_timestamp_ms']) / 1000\n",
    "            out += \" ({} seconds)\".format(timeSecs)\n",
    "        stati.append(out)\n",
    "    return stati\n",
    "\n",
    "def cancelTasks(taskList):\n",
    "    for t in taskList:\n",
    "        status = t.status()\n",
    "        if status['state'] == 'RUNNING' or status['state'] == 'READY':\n",
    "            print(\"Cancelling {}\".format(status['description']))\n",
    "            t.cancel()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage\n",
    "\n",
    "First create the collection in the way you want to export, with images that have a single band. This could be the images as-is, or some kind of temporal summary such as annual images. \n",
    "\n",
    "Then call the appropriate getCollExportParams_* function to get a list of parameter dictionaries, one for each image to be exported.\n",
    "\n",
    "Then create tasks from each set of parameters using createTasks(). \n",
    "\n",
    "Then start those tasks with performTasks()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIIRS\n",
    "\n",
    "* Continuous dataset, monthly, extent 75N-65S\n",
    "* Actually we're going to take the opportunity to change this to 75N-60S to slightly reduce storage\n",
    "* Native resolution equivalent to 500m (but not precisely equal to the mastergrids 500m, so use the getCollExportParams_ContAgg function to allow us to specify the required MG resolution as opposed to outputting the exact native resolution but specify \"Data\" for the aggregations to just use NN resampling\n",
    "* For aggregated data, we only store this at 5km aggregated resolution, not 1km or 10km, and we will want mean, min, max, SD.\n",
    "\n",
    "We're just exporting the monthly files as-is so all we need to do is select the band and filter to the dates we want. So first create the collection for export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "pfhr1W_k8t4x"
   },
   "outputs": [],
   "source": [
    "coll_viirs = (ee.ImageCollection(\"NOAA/VIIRS/DNB/MONTHLY_V1/VCMSLCFG\")\n",
    "    .map(lambda i: (i.select('avg_rad')))\n",
    "    .filter(ee.Filter.date(ee.Date.fromYMD(2017,3,1),\n",
    "                           ee.Date.fromYMD(2017,9,30))))\n",
    "exportColl = coll_viirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 500m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "xjRxS814xYog"
   },
   "outputs": [],
   "source": [
    "exportRegion = mgRegions['viirs-extent-7560']\n",
    "exportRes = mgResolutions['500m']\n",
    "exportItems = getCollExportParams_ContAgg(collection=exportColl, mgTimeStepName=\"Monthly\", timeSummaryName=\"Data\",\n",
    "                                       mgResolutionName=\"500m\", variableName=\"VIIRS-SLC\", regionGeom=exportRegion,\n",
    "                                       aggregations=[\"Data\"])\n",
    "exportItems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3945
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 563,
     "status": "ok",
     "timestamp": 1559735056751,
     "user": {
      "displayName": "Harry Gibson",
      "photoUrl": "",
      "userId": "07981519042514161991"
     },
     "user_tz": -60
    },
    "id": "5nqR7jEiMhq3",
    "outputId": "5c5f9d68-4e69-4301-c49f-da035b4955d9"
   },
   "outputs": [],
   "source": [
    "viirs500m_tasks = createTasks(exportItems)\n",
    "# just a bit of faffing if the exports fail and we have to re-run\n",
    "# errortasks = [t.status()['description'] for t in viirs500m_tasks if t.status()['state']==\"FAILED\"]\n",
    "# viirs500m_tasks = [t for t in viirs500m_tasks_new if str(t).split(' ')[2] in errortasks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Task EXPORT_IMAGE: VIIRS-SLC_2017_03_Data_500m_Data (UNSUBMITTED)>,\n",
       " <Task EXPORT_IMAGE: VIIRS-SLC_2017_04_Data_500m_Data (UNSUBMITTED)>,\n",
       " <Task EXPORT_IMAGE: VIIRS-SLC_2017_05_Data_500m_Data (UNSUBMITTED)>,\n",
       " <Task EXPORT_IMAGE: VIIRS-SLC_2017_06_Data_500m_Data (UNSUBMITTED)>,\n",
       " <Task EXPORT_IMAGE: VIIRS-SLC_2017_07_Data_500m_Data (UNSUBMITTED)>,\n",
       " <Task EXPORT_IMAGE: VIIRS-SLC_2017_08_Data_500m_Data (UNSUBMITTED)>,\n",
       " <Task EXPORT_IMAGE: VIIRS-SLC_2017_09_Data_500m_Data (UNSUBMITTED)>]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note that the default str of the task seems to show UNSUBMITTED even when it's running\n",
    "viirs500m_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "performTasks(viirs500m_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VIIRS-SLC_2017_03_Data_500m_Data: FAILED',\n",
       " 'VIIRS-SLC_2017_04_Data_500m_Data: FAILED',\n",
       " 'VIIRS-SLC_2017_05_Data_500m_Data: FAILED',\n",
       " 'VIIRS-SLC_2017_06_Data_500m_Data: COMPLETED',\n",
       " 'VIIRS-SLC_2017_07_Data_500m_Data: FAILED',\n",
       " 'VIIRS-SLC_2017_08_Data_500m_Data: FAILED',\n",
       " 'VIIRS-SLC_2017_09_Data_500m_Data: FAILED']"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so instead:\n",
    "reportTasks(viirs500m_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cancelling VIIRS-SLC_2017_10_Data_500m_Data\n",
      "Cancelling VIIRS-SLC_2017_11_Data_500m_Data\n",
      "Cancelling VIIRS-SLC_2018_03_Data_500m_Data\n",
      "Cancelling VIIRS-SLC_2018_04_Data_500m_Data\n",
      "Cancelling VIIRS-SLC_2018_07_Data_500m_Data\n",
      "Cancelling VIIRS-SLC_2018_08_Data_500m_Data\n",
      "Cancelling VIIRS-SLC_2018_09_Data_500m_Data\n",
      "Cancelling VIIRS-SLC_2018_10_Data_500m_Data\n",
      "Cancelling VIIRS-SLC_2018_11_Data_500m_Data\n",
      "Cancelling VIIRS-SLC_2018_12_Data_500m_Data\n"
     ]
    }
   ],
   "source": [
    "#cancelTasks(viirs500m_tasks_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "xjRxS814xYog"
   },
   "outputs": [],
   "source": [
    "exportColl = coll_viirs\n",
    "exportRegion = mgRegions['viirs-extent-7560']\n",
    "exportOrigin = getOriginXY(exportRegion)\n",
    "exportRes = mgResolutions['5km']\n",
    "exportAgg = None\n",
    "\n",
    "#collection, mgTimeStep, timeSummaryName, mgResolutionName, \n",
    " #                              varName, regionGeom, aggregations=[\"Unchanged\"])\n",
    "exportItems = getCollectionExportParams(collection=exportColl, mgTimeStep=\"Monthly\", timeSummaryName=\"Data\",\n",
    "                                       mgResolutionName=\"5km\", varName=\"VIIRS-SLC\", regionGeom=exportRegion,\n",
    "                                       aggregations=[\"min\", \"max\", \"mean\", \"SD\"])\n",
    "exportItems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "viirs5km_tasks = createTasks(exportItems)\n",
    "viirs5km_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "performTasks(viirs5km_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def downloadImages(taskList, localFolder):\n",
    "    for t in taskList:\n",
    "        if 1: #t.status()['state']=='COMPLETED':\n",
    "            prefix = t.config['outputPrefix']\n",
    "            gsUrl = \"gs://{}/{}*\".format(EXPORT_BUCKET, prefix)\n",
    "            gsCmd = \"gsutil -m mv {} {}\".format(gsUrl, localFolder)\n",
    "            print(gsCmd)\n",
    "            !{gsCmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'outputPrefix'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-c621054564b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdownloadImages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmpTasks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mr'C:\\Temp\\dataprep\\VIIRS\\5km'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-cf8637d97e19>\u001b[0m in \u001b[0;36mdownloadImages\u001b[1;34m(taskList, localFolder)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtaskList\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'state'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'COMPLETED'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m             \u001b[0mprefix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'outputPrefix'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m             \u001b[0mgsUrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"gs://{}/{}*\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEXPORT_BUCKET\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[0mgsCmd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"gsutil -m mv {} {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgsUrl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocalFolder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'outputPrefix'"
     ]
    }
   ],
   "source": [
    "downloadImages(tmpTasks, r'C:\\Temp\\dataprep\\VIIRS\\5km')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synoptic MODIS\n",
    "\n",
    "A bit more pre-processing here to create temporal aggregations.\n",
    "\n",
    "Here we'll set the variable name (used to create the first part of output filenames) directly on each image \n",
    "so we can make a single call to the getCollExportParams_ContAgg, rather than having to call it once per \n",
    "varname."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_for_synoptic_month(coll, month):\n",
    "    cMonthColl = coll.filter(ee.Filter.calendarRange(month, month, 'month'))\n",
    "    return summariseColl(cMonthColl)\n",
    "    \n",
    "def mean_of_synoptic_monthly_means(coll):\n",
    "    monthly_means = []\n",
    "    for month in range(1, 13):\n",
    "        monthly_means.append(mean_for_synoptic_month(coll, month))\n",
    "    return ee.ImageCollection(monthly_means).mean()\n",
    "\n",
    "def summariseColl(coll):\n",
    "    mean = coll.reduce(ee.Reducer.mean())\n",
    "    sd = coll.reduce(ee.Reducer.sampleStdDev())\n",
    "    iMin = coll.reduce(ee.Reducer.min())\n",
    "    iMax = coll.reduce(ee.Reducer.max())\n",
    "    allIm = ee.Image.cat(mean, sd, iMin, iMax)\n",
    "    return allIm\n",
    "\n",
    "def monthlySummary(coll):\n",
    "    firstDate = ee.Date(coll.aggregate_min('system:time_start'))\n",
    "    lastDate = ee.Date(coll.aggregate_max('system:time_start'))\n",
    "    startMonth = ee.Date.fromYMD(firstDate.get('year'), firstDate.get('month'), 1)\n",
    "    endMonth = ee.Date.fromYMD(lastDate.get('year'), lastDate.get('month'), 1)\n",
    "    # difference is based on \"average\" month so this won't necessarily be an int\n",
    "    nSteps = endMonth.difference(startMonth, 'month').round()\n",
    "    print(nSteps.getInfo())\n",
    "    dates = ee.List.sequence(0, nSteps, 1).map(lambda n: startMonth.advance(n, 'month'))\n",
    "    print(dates.getInfo())\n",
    "    monthlyIms = dates.map(lambda d: summariseColl(\n",
    "        coll.filter(ee.Filter.date(ee.DateRange(d, ee.Date(d).advance(1,'month')))))\n",
    "                          .set('system:time_start', ee.Date(d).millis())\n",
    "                          .set('system:time_end', ee.Date(d).advance(1, 'month').millis()))\n",
    "    return monthlyIms\n",
    "    \n",
    "def annual_means(coll):\n",
    "    firstDate = ee.Date(coll.aggregate_min('system:time_start'))\n",
    "    lastDate = ee.Date(coll.aggregate_max('system:time_start'))\n",
    "    startYear = ee.Date.fromYMD(firstDate.get('year'), 1, 1)\n",
    "    endYear = ee.Date.fromYMD(lastDate.get('year'), 1, 1)\n",
    "    nSteps = endYear.difference(startYear, 'year').round()\n",
    "    dates = ee.List.sequence(0, nSteps, 1).map(lambda n: startYear.advance(n, 'year'))\n",
    "    annualIms = dates.map(lambda d: summariseColl(\n",
    "        coll.filter(ee.Filter.date(ee.DateRange(d, ee.Date(d).advance(1, 'year')))))\n",
    "                          .set('system:time_start', ee.Date(d).millis())\n",
    "                          .set('system:time_end', ee.Date(d).advance(1, 'year').millis()))\n",
    "    return annualIms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod11a2 = ee.ImageCollection('MODIS/006/MOD11A2')\n",
    "mod11a2 = mod11a2.filter(ee.Filter.calendarRange(2000, 2017, 'year'))\n",
    "lst_day_c_coll = mod11a2.map(lambda i: i.select('LST_Day_1km')\n",
    "                             .multiply(0.02).subtract(273.15).\n",
    "                             set('system:time_start', i.get('system:time_start')))\n",
    "lst_night_c_coll = mod11a2.map(lambda i: i.select('LST_Night_1km')\n",
    "                               .multiply(0.02).subtract(273.15)\n",
    "                               .set('system:time_start', i.get('system:time_start')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'Image',\n",
       " 'bands': [{'id': 'LST_Day_1km_mean',\n",
       "   'data_type': {'type': 'PixelType',\n",
       "    'precision': 'double',\n",
       "    'min': -273.15,\n",
       "    'max': 1037.5500000000002},\n",
       "   'crs': 'EPSG:4326',\n",
       "   'crs_transform': [1.0, 0.0, 0.0, 0.0, 1.0, 0.0]},\n",
       "  {'id': 'LST_Day_1km_stdDev',\n",
       "   'data_type': {'type': 'PixelType', 'precision': 'double'},\n",
       "   'crs': 'EPSG:4326',\n",
       "   'crs_transform': [1.0, 0.0, 0.0, 0.0, 1.0, 0.0]},\n",
       "  {'id': 'LST_Day_1km_min',\n",
       "   'data_type': {'type': 'PixelType',\n",
       "    'precision': 'double',\n",
       "    'min': -273.15,\n",
       "    'max': 1037.5500000000002},\n",
       "   'crs': 'EPSG:4326',\n",
       "   'crs_transform': [1.0, 0.0, 0.0, 0.0, 1.0, 0.0]},\n",
       "  {'id': 'LST_Day_1km_max',\n",
       "   'data_type': {'type': 'PixelType',\n",
       "    'precision': 'double',\n",
       "    'min': -273.15,\n",
       "    'max': 1037.5500000000002},\n",
       "   'crs': 'EPSG:4326',\n",
       "   'crs_transform': [1.0, 0.0, 0.0, 0.0, 1.0, 0.0]}]}"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summariseColl(lst_day_c_coll).getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceBandName = \"LST_Day_1km\"\n",
    "lst_day_synoptic = summariseColl(lst_day_c_coll)\n",
    "lst_day_mean = lst_day_synoptic.select(sourceBandName+\"_mean\").set('varname', 'AFR_LST_Day_v6')\n",
    "lst_day_sd = lst_day_synoptic.select(sourceBandName+\"_stdDev\").set('varname', 'AFR_LST_Day_v6')\n",
    "lst_day_min = lst_day_synoptic.select(sourceBandName+\"_min\").set('varname', 'AFR_LST_Day_v6')\n",
    "lst_day_max = lst_day_synoptic.select(sourceBandName+\"_max\").set('varname', 'AFR_LST_Day_v6')\n",
    "lst_day_balanced_mean = mean_of_synoptic_monthly_means(lst_day_c_coll).set('varname', 'AFR_LST_Day_v6')\n",
    "lst_night_mean = lst_night_c_coll.mean().set('varname', 'AFR_LST_Night_v6')\n",
    "lst_night_sd = lst_night_c_coll.reduce(ee.Reducer.sampleStdDev()).set('varname', 'AFR_LST_Night_v6')\n",
    "lst_night_balanced_mean = mean_of_synoptic_monthly_means(lst_night_c_coll).set('varname', 'AFR_LST_Night_v6')\n",
    "\n",
    "sdColl = ee.ImageCollection.fromImages([lst_day_sd, lst_night_sd])\n",
    "meanColl = ee.ImageCollection.fromImages([lst_day_mean, lst_night_mean])\n",
    "balancedMeanColl = ee.ImageCollection.fromImages([lst_day_balanced_mean, lst_night_balanced_mean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "exportRegion = mgRegions['mapAfrica']\n",
    "\n",
    "exportItemsSD = getCollExportParams_ContAgg(collection=sdColl, mgTimeStepName=\"Synoptic_Overall\", timeSummaryName=\"SD\",\n",
    "                                         mgResolutionName=\"1km\", variableName=None, regionGeom=exportRegion,\n",
    "                                         aggregations=[\"Data\"])\n",
    "exportItemsMean = getCollExportParams_ContAgg(collection=meanColl, mgTimeStepName=\"Synoptic_Overall\", timeSummaryName=\"mean\",\n",
    "                                         mgResolutionName=\"1km\", variableName=None, regionGeom=exportRegion,\n",
    "                                         aggregations=[\"Data\"])\n",
    "exportItemsBalancedMean = getCollExportParams_ContAgg(collection=balancedMeanColl, mgTimeStepName=\"Synoptic_Overall\", timeSummaryName=\"Balanced-mean\",\n",
    "                                         mgResolutionName=\"1km\", variableName=None, regionGeom=exportRegion,\n",
    "                                         aggregations=[\"Data\"])\n",
    "\n",
    "# this syntax only works in python 3.5+\n",
    "exportItems = {**exportItemsSD, **exportItemsMean, **exportItemsBalancedMean}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=createTasks(exportItems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "performTasks(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LST_Day_v6_Synoptic_Overall_SD_1km_Data: COMPLETED (7642.124 seconds)',\n",
       " 'LST_Night_v6_Synoptic_Overall_SD_1km_Data: COMPLETED (8075.524 seconds)',\n",
       " 'LST_Day_v6_Synoptic_Overall_mean_1km_Data: RUNNING (8250.14 seconds)',\n",
       " 'LST_Night_v6_Synoptic_Overall_mean_1km_Data: COMPLETED (2561.55 seconds)',\n",
       " 'LST_Day_v6_Synoptic_Overall_Balanced-mean_1km_Data: COMPLETED (2191.036 seconds)',\n",
       " 'LST_Night_v6_Synoptic_Overall_Balanced-mean_1km_Data: COMPLETED (2353.463 seconds)']"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reportTasks(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCD12Q1 Landcover (IGBP)\n",
    "\n",
    "This is categorical data. We want to export the original images as-is, then for the spatial aggregations we will export, from each input image, a single majority-class image and one fraction-cover image for each input class (value). \n",
    "\n",
    "First, the original collection contains multiple different landcover classification schemes; we are only interested in the IGBP one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ce8gFO4G7tEk"
   },
   "outputs": [],
   "source": [
    "coll_mcd12q1 = ee.ImageCollection(\"MODIS/006/MCD12Q1\")\n",
    "coll_igbp = coll_mcd12q1.map(lambda i: (i.select('LC_Type1')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 13947,
     "status": "ok",
     "timestamp": 1559734893059,
     "user": {
      "displayName": "Harry Gibson",
      "photoUrl": "",
      "userId": "07981519042514161991"
     },
     "user_tz": -60
    },
    "id": "NvOpLgkqMMxw",
    "outputId": "3d58b280-361f-4c47-8f9f-bce500f535aa"
   },
   "outputs": [],
   "source": [
    "#collection, mgTimeStep, timeSummaryName, mgResolutionName, \n",
    "#                               varName, regionGeom, aggregations=[\"Unchanged\"]):\n",
    "    \n",
    "igbp_500m_items = getCollExportParams_CatAgg(collection=coll_igbp, \n",
    "                                            mgTimeStepName=\"Annual\", timeSummaryName=\"Data\", \n",
    "                                            mgResolutionName=\"500m\", variableName=\"IGBP_Landcover\",\n",
    "                                            regionGeom=mgRegions['global'],\n",
    "                                            aggregations='Data',\n",
    "                                            categoryMapping=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "igbp_500m_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "igbp_500m_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "igbpTasks = createTasks(igbp_500m_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "performTasks(igbpTasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reportTasks(igbpTasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5km\n",
    "\n",
    "For producing the class fractions, we need to know what values to look for, and what names (if different) these values should have. Get this from the metadata on a single image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 2116,
     "status": "ok",
     "timestamp": 1559748817886,
     "user": {
      "displayName": "Harry Gibson",
      "photoUrl": "",
      "userId": "07981519042514161991"
     },
     "user_tz": -60
    },
    "id": "nALxrAJtMszk",
    "outputId": "dbe6b8e9-1312-4ef2-ccf9-45f5df998807"
   },
   "outputs": [],
   "source": [
    "test_landcover = coll_igbp.first()\n",
    "catVals = test_landcover.get('LC_Type1_class_values').getInfo()\n",
    "catNames = test_landcover.get('LC_Type1_class_names').getInfo()\n",
    "catNamesClean = [n.split(':')[0].replace(' ', '_').replace('/','_') for n in catNames]\n",
    "catRefs = [\"IGBP_Landcover_Class\"+str(cat).zfill(2) for cat in catVals]\n",
    "fileNames = [c + \"_\" + n for c,n in zip(catRefs, catNamesClean)]\n",
    "values_with_names = list(zip(catVals, fileNames))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "igbp_5km_items = getCollExportParams_CatAgg(collection=coll_igbp,\n",
    "                                           mgTimeStepName=\"Annual\", timeSummaryName=\"Data\",\n",
    "                                           mgResolutionName=\"5km\", variableName=\"IGBP_Landcover\",\n",
    "                                           regionGeom=mgRegions['global'],\n",
    "                                           aggregations=['majority', 'fractions'],\n",
    "                                           categoryMapping=values_with_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'IGBP_Landcover_Class01_Evergreen_Needleleaf_Forests'),\n",
       " (2, 'IGBP_Landcover_Class02_Evergreen_Broadleaf_Forests'),\n",
       " (3, 'IGBP_Landcover_Class03_Deciduous_Needleleaf_Forests'),\n",
       " (4, 'IGBP_Landcover_Class04_Deciduous_Broadleaf_Forests'),\n",
       " (5, 'IGBP_Landcover_Class05_Mixed_Forests'),\n",
       " (6, 'IGBP_Landcover_Class06_Closed_Shrublands'),\n",
       " (7, 'IGBP_Landcover_Class07_Open_Shrublands'),\n",
       " (8, 'IGBP_Landcover_Class08_Woody_Savannas'),\n",
       " (9, 'IGBP_Landcover_Class09_Savannas'),\n",
       " (10, 'IGBP_Landcover_Class10_Grasslands'),\n",
       " (11, 'IGBP_Landcover_Class11_Permanent_Wetlands'),\n",
       " (12, 'IGBP_Landcover_Class12_Croplands'),\n",
       " (13, 'IGBP_Landcover_Class13_Urban_and_Built-up_Lands'),\n",
       " (14, 'IGBP_Landcover_Class14_Cropland_Natural_Vegetation_Mosaics'),\n",
       " (15, 'IGBP_Landcover_Class15_Permanent_Snow_and_Ice'),\n",
       " (16, 'IGBP_Landcover_Class16_Barren'),\n",
       " (17, 'IGBP_Landcover_Class17_Water_Bodies')]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values_with_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "igbp_5km_items.keys)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "igbp5kmTasks = createTasks(igbp_5km_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "performTasks(igbp5kmTasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reportTasks(igbp5kmTasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancelTasks(igbp5kmTasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Extract_MG_Aligned_Covariates",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
